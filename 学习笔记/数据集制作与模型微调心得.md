## PythonAIFinal轮-------使用模型基准测试心得

本轮是本方向考核的最后一轮，我们的任务是学习数据集的制作和使用已有的预训练模型进行下游任务的微调
考核任务
1小说续写数据集制作：使用 python 爬虫爬取小说网站获取一些小说文本，并将其制作成数据集
 FictionDataset ( Torch . Dataset 的派生类），爬取时注意平衡小说种类。
2基于 transformer 库的 bert - base - chinese 进行下游训练使其能够续写小说，且逻辑通顺
3（可选）使用 Flask 库为模型构建后端服务器，使模型能够部署到 web 应用上。

## 1.爬小说！

### 1.1小说网站的选择

当然是免费小说网站啦（运气不好之前选了两个小说网站都在不久之后崩溃了QWQ）

这是我爬的网站对象：https://quanben5.com/



### 1.2爬取代码与筛选小说

我们需要得到的信息有小说的分类（避免文风倾向性过于明显）与正文（用来训练）这里用了beautifulsoup库

```py
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
import time

headers = {
    #'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
    'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 11_0 like Mac OS X) AppleWebKit/604.1.34 (KHTML, like Gecko) Version/11.0 Mobile/15A5341f Safari/604.1'

}                                  #-------------多换几个请求的头免得被定点打击

#每章文字的爬取
def getASection(url, bookName):
    bookName += ".txt"
    f = open(bookName, "a", encoding='utf-8')
    rsp = requests.get(url, headers=headers)
    rsp.encoding = 'utf-8'
    bs = BeautifulSoup(rsp.text, 'html.parser')     #简单的beautifulsoup调用

    title = bs.select('h1')					        #标题被封装在<h1></h1>里
    if not title:
        print("无法找到章节标题：", url)
        return

    f.write(title[0].text)
    f.write("\n")

    body = bs.select('div.content')                  #文章被封装在<content></content>里
    if not body:
        print("无法找到章节内容：", url)
        return

    paragraphs = body[0].find_all('p')               #正文被封装在<p></p>里
    content = []
    for p in paragraphs:
        content.append(p.text)
        content.append("\n")					     #便于文本正常读入

    f.writelines(content)


    f.close()

#每本书每一章节对应链接的爬取
def getSections(url, bookName):
    rsp = requests.get(url, headers=headers)
    rsp.encoding = 'utf-8'
    bs = BeautifulSoup(rsp.text, 'html.parser')
    sections = bs.select('ul.list')[0]
    links = sections.select('a')
    #print(links)
    for link in links:
        if link.attrs["href"] is not None:
            newUrl = urljoin(url, link.attrs['href'])#链接被封装在<a herf>里
            getASection(newUrl, bookName)

#每本书的分类以及书的主页网址的爬取
def getBooks(url):
    bookUrls = dict()                       # 用字典来存放小说信息，键为书名，值为链接地址
    rsp = requests.get(url, headers=headers)
    rsp.encoding = 'utf-8'
    bs = BeautifulSoup(rsp.text, 'html.parser')
    for j in range(6):                         #这个网站一共有6个分类
        time.sleep(10)
        bookList = bs.select('div.c3')[j]
        sort = bookList.select('h2.title')[0] 
        sort = sort.find('span').text          #分类被封装在<div class=c3>中的<h2 class=title><span>内容里
        for i in range(5):                     #每个分类爬5本够了，多了练不完
            bookList = bs.select('div.c3')[j]
            book = bookList.select('a')[i]
            if book.attrs['href'] is not None:
                href = 'https://quanben5.com/' + book.attrs['href'] + 'xiaoshuo.html'
                #把href改为可以直接进入网站点的网址
                href = href.replace('book', 'list')  # 需要把url中的book替换为list以直接进入章节页面
                bookName = '(' +sort + ')' + book.text  #便于对小说分类(一会要用)
                if bookName not in bookUrls:
                    bookUrls[bookName] = href
                    print("{}:{}".format(bookName,href))
        for bookName in bookUrls.keys():
            getSections(bookUrls[bookName], bookName)
            print('{}已经完成'.format(bookName))         #成功爬取的提示


getBooks('https://quanben5.com/')                 #(或许可以试试scapy？)
```

一共有6个分类:玄幻，都市，言情，武侠，仙侠，穿越

爬完小说部分过程如下：

![爬到的小说](C:\Users\Lenovo\Desktop\py\AI考核final\爬到的小说.png)



最后我发现小说之间的文件大小差距过大：玄幻（210MB），都市（47MB），言情（20MB），武侠（97MB），仙侠（40MB），穿越（30MB）看来只能人为选择了，不然文风倾向性过于明显了。



### 1.3数据清洗

```py
def remove_keywords_and_lines(input_file, output_file):
    keywords = ['上一页', '目录', '下一页']

    # 打开输入文件并读取内容
    with open(input_file, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    # 移除上述字符和包含'（）'字符的行
    filtered_lines = [line for line in lines if all(keyword not in line for keyword in keywords) and '()' not in line]

    # 打开输出文件并写入处理后的内容
    with open(output_file, 'w', encoding='utf-8') as file:
        file.writelines(filtered_lines)

if __name__ == "__main__":
    input_file_path = "(穿越)大蛇王.txt"   # 输入文件路径
    output_file_path = "(穿越)大蛇王.txt" # 输出文件路径

    remove_keywords_and_lines(input_file_path, output_file_path)

```





## 2.建数据集！

为了方便pytorch直接调用Dataloader，我们依据Torch . Dataset 的派生类来建立我们的数据集（继承torch.util.data.Dataset）

### 2.1定义数据集

步骤：

1.继承torch.util.data.Dataset

2.实现__getitem__方法

3.实现__len__方法

```py
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForMaskedLM

class FictionDataset(Dataset):
    #继承torch.util.data.Dataset(包括self,文件路径txt_file,最大长度max_length)
    def __init__(self, txt_file, max_length=128):
        self.data = []  # 存储文本数据的列表
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')#直接使用该预训练模型的tokenizer以方便回头训练时直接调用
        self.max_length = max_length

        # 从TXT文件中读取数据
        with open(txt_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if len(line) > 0:
                    self.data.append(line)
                    
	#实现__len__方法(包括self)
    def __len__(self):
        return len(self.data)
    
	#实现__getitem__方法(包括self,数据标签idx)
    def __getitem__(self, idx):
        text = self.data[idx]

        # 使用BertTokenizer对文本进行分词，并添加特殊标记[CLS]和[SEP]
        # truncation=True 表示截断文本以适配指定的max_length
        # padding='max_length' 表示对文本进行填充以适配指定的max_length
        # return_tensors='pt' 表示返回PyTorch张量
        encoded = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length',truncation=True, return_tensors='pt')

        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        token_type_ids = encoded['token_type_ids'].squeeze()
        
        #返回一个集合，包括
        return {
            'input_ids': input_ids,               #输入的张量
            'attention_mask': attention_mask,     #注意力掩层
            'token_type_ids': token_type_ids      #对应的索引
        }
```



### 2.2构建，保存与完善数据集

这里我使用pickle库维护数据集

```py
# 定义文件路径
dataset_save_path = "MyDataset.pkl"

#1读取文本数据并构建数据集
txt_file_path = "(穿越)大蛇王.txt"
dataset = FictionDataset(txt_file=txt_file_path, max_length=128)
   
```

```py
#2使用 pickle 将 FictionDataset 保存到文件
with open(dataset_save_path, 'wb') as f:
    pickle.dump(dataset, f)

```

```py
#3读取文本数据并添入数据集
new_txt_file_path = "(穿越)妃子令，冥王的俏新娘.txt"
new_dataset = FictionDataset(txt_file=new_txt_file_path, max_length=128)

#3.1加载 FictionDataset
with open(dataset_save_path, 'rb') as f:
    loaded_dataset = pickle.load(f)

#3.2将新数据添加到已加载的数据集中
loaded_dataset.data.extend(new_dataset.data)

#3.3使用 pickle重新保存更新后的 FictionDataset
with open(dataset_save_path, 'wb') as f:
    pickle.dump(loaded_dataset, f)

```



### 2.3加载并使用数据集

我将按以下操作进行数据集加载

```py
# 加载 FictionDataset
with open('MyFictionDataset.pkl', 'rb') as f:
    dataset = pickle.load(f)
    
# 构建数据加载器
batch_size = 16
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)#shuffle: 如果设置为 True时在每个 epoch 时随机打乱数据。  
```

这样以后就可以直接用

```py
    for batch in data_loader:
        inputs = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
```

进行调用





## 3.对模型的下游训练

在做完数据集之后当然就是训练模型啦

### 3.1加载模型

```py
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForMaskedLM
import torch.optim as optim
import torch.nn as nn
from FictionDatasetMake import FictionDataset
import pickle


# 读取文本数据并构建数据集
#txt_file_path = "大蛇王(穿越).txt"
#dataset = FictionDataset(txt_file=txt_file_path, max_length=128)

# 加载 FictionDataset
with open('MyFictionDataset.pkl', 'rb') as f:
    dataset = pickle.load(f)

# 构建数据加载器
batch_size = 16
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 初始化并加载bert-base-chinese模型（下面那个是重新加载并训练）
model = BertForMaskedLM.from_pretrained('bert-base-chinese')
#load_path = "BertModel.pt"
#model.load_state_dict(torch.load(load_path))

# 锁定模型的底层层（例如前6层）不参与参数更新
for param in model.bert.encoder.layer[:6].parameters():
    param.requires_grad = False
    
# 定义优化器和损失函数
criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数
optimizer = optim.Adam(model.parameters(), lr=2e-5)

# 将模型放置在GPU或CPU上
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)


```



### 3.2训练模型

#### 3.2.1 首先是正常的将目标标签进行移动并预测的训练方法

（其实这个预训练模型仅仅只要训练6轮就能使loss降到很低的程度，虽然也花了不少时间）这个训练结果被保存为"BretModel-Hard.pt"难一点的与"BretModel-Simple.pt"简单的

```py
# 进行下游训练
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    num_batches = len(data_loader)  # 获取总批次数
    for batch_idx, batch in enumerate(data_loader):
        #加载三个模型系数:输入数据,注意力掩码,类型ID
        inputs = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)

        # 正向传播，获取模型输出
        outputs = model(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # 从输出中获取预测的logits，它是一个三维张量，包含了模型对每个位置上词汇表中每个单词的预测概率。
        # 包括维度通常为 [batch_size, max_length, vocab_size]。
        logits = outputs.logits

        # 将输入的目标标签移动一位，用于计算损失
        labels = inputs.clone()				# 复制输入数据作为目标标签
        labels[attention_mask == 0] = -100  # 将填充部分的目标标签设置为-100，这样在计算损失时会忽略填充位置。
        labels = labels[:, 1:].contiguous() # 将目标标签移动一位labels是目标标签的张量，通过切片[:, 1:]取出除了第一个位置以外的所有位置的标签。然后，通过调用contiguous()方法，获得了一个连续内存的副本，确保在计算损失时维度是对齐的。

        # 计算损失1（这个训练结果被保存为BretModel-Hard.pt）
        loss = criterion(logits[:, :-1, :].contiguous().view(-1, model.config.vocab_size), labels.view(-1))  # 计算损失，这里需要将logits和标签labels都做相应的维度变换，确保对齐计算损失。
        total_loss += loss.item()
        
        # 计算损失2(以这里定义的损失函数加载的模型为Bret-Model-simple.pt)
        loss = criterion(logits.view(-1, model.config.vocab_size), inputs.view(-1))
        total_loss += loss.item()

        # 反向传播和优化模型参数
        optimizer.zero_grad()               # 梯度清零
        loss.backward()                     # 反向传播
        optimizer.step()                    # 模型参数更新

        if batch_idx % 100 == 1:
            # 打印当前批次进度
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}")

    # 计算平均损失
    avg_loss = total_loss / len(data_loader)

    # 打印训练进度
    print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

# 保存最终的模型
save_path = "BretModel-Hard.pt"
torch.save(model.state_dict(), save_path)
print('OK!')
```

新增知识：

```py
# 从输出中获取预测的logits，它是一个三维张量，包含了模型对每个位置上词汇表中每个单词的预测概率。
        # 包括维度通常为 [batch_size, max_length, vocab_size]。
        logits = outputs.logits

        # 将输入的目标标签移动一位，用于计算损失
        labels = inputs.clone()				# 复制输入数据作为目标标签
        labels[attention_mask == 0] = -100  # 将填充部分的目标标签设置为-100，这样在计算损失时会忽略填充位置。
        labels = labels[:, 1:].contiguous() # 将目标标签移动一位labels是目标标签的张量，通过切片[:, 1:]取出除了第一个位置以外的所有位置的标签。然后，通过调用contiguous()方法，获得了一个连续内存的副本，确保在计算损失时维度是对齐的。

        # 计算损失
        loss = criterion(logits[:, :-1, :].contiguous().view(-1, model.config.vocab_size), labels.view(-1))  # 计算损失，这里需要将logits和标签labels都做相应的维度变换，确保对齐计算损失。
```

####  3.2.2然后是n-gram训练方法



首先要重写FictionDataset中的getitem方法

```py
    def __getitem__(self, idx):
        text = self.data[idx]

        # 将输入的文本根据n-gram拆分为多个样本
        samples = []
        for i in range(len(text) - self.n_gram):
            sample_text = text[i:i+self.n_gram]
            encoded = self.tokenizer.encode_plus(sample_text, max_length=self.max_length, padding='max_length',
                                                 truncation=True, return_tensors='pt')
            input_ids = encoded['input_ids'].squeeze()
            attention_mask = encoded['attention_mask'].squeeze()
            token_type_ids = encoded['token_type_ids'].squeeze()

            samples.append({
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'token_type_ids': token_type_ids
            })
```

然后就可以正常运行训练了：

```py
 在训练循环中，根据新的数据集样本计算loss
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    num_batches = len(data_loader)  # 获取总批次数
    for batch_idx, batch in enumerate(data_loader):
        for inputs in batch:
            inputs = {k: v.to(device) for k, v in inputs.items()}
            targets = inputs['input_ids'][:, -1].clone()
            inputs['input_ids'] = inputs['input_ids'][:, :-1]
            inputs['attention_mask'] = inputs['attention_mask'][:, :-1]
            inputs['token_type_ids'] = inputs['token_type_ids'][:, :-1]
            loss = compute_loss(model, inputs, targets)
            total_loss += loss.item()

            # 反向传播和优化模型参数
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

           # if batch_idx % 10 == 1:
                # 打印当前批次进度
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}")
```







## 4.模型的使用与结果的展示

### 4.1 使用beam方法进行小说文本续写(效果真的不太行)

我们来看看集束搜索（beam search）算法，上篇文章选择最可能的句子讲了对于机器翻译来说，给定输入（法语句子），我们并不想要一个随机的英语翻译结果，而是想要一个最好的，最可能的英语翻译结果。对于语音识别也一样，给定一个输入的语音片段，我们不会想要一个随机的文本翻译结果，而是想要最接近原意的翻译结果，集束搜索就是解决这个最常用的算法。让我们用法语句子的例子来试一下集束搜索吧。



“Jane visite l'Afrique en Septembre.”（法语句子），把它翻译成英语，"Jane is visiting Africa in September."（英语句子），（1）集束搜索算法首先做的就是挑选要输出的英语翻译中的第一个（first）单词。这里Andrew使用10,000个词的词汇表，为了简化问题，这里忽略大小写，所有的单词都以小写列出来。在集束搜索的第一步中用的这个网络部分，绿色是编码部分（encoder），紫色是解码部分（decoder），来评估第一个单词的概率值，给定输入序列x，即法语作为输入，第一个输出y的概率值是多少。

贪婪算法只会挑出最可能的那一个单词，然后继续。而集束搜索则会考虑多个选择，集束搜索算法会有一个参数B，叫做集束宽（beam width）。在这个例子中B=3，这样就意味着集束搜索不会只考虑一个可能结果，而是一次会考虑3个，比如对第一个单词有不同选择的可能性，最后找到in、jane、september，是英语输出的第一个单词的最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。如果集束宽设的不一样，如果集束宽这个参数是10的话，那么我们跟踪的不仅仅3个，而是10个第一个单词的最可能的选择。所以要明白，为了执行集束搜索的第一步，你需要输入法语句子到编码网络，然后会解码这个网络，这个softmax层（紫色网络的蓝框）会输出10,000个概率值，得到这10,000个输出的概率值，取前三个存起来。



让我们看看集束搜索算法的第二步，已经选出了第一个单词三个最可能的选择为in、jane、september，集束算法接下来会针对每个第一个单词考虑第二个单词分别是什么，如上图蓝色标记，单词in后面的第二个单词可能是a或者aaron，从词汇表里把这些词列了出来，也可能是september、 visit和z，最后一个单词是zulu。

为了评估第二个词的概率值，我们用这个神经网络，其中绿色是编码部分，紫色是解码部分，当决定单词in后面是什么，别忘了解码器的第一个输出y^<1>是单词in，然后把它喂回来，下一个节点的输入就是单词in，输出是y^<2>，有了这个连接，这个网络就可以用来评估在翻译结果的第一个单词in的情况下第二个单词的概率。

注意，在第二步里我们更关心的是要找到最可能的第一个和第二个单词对，即第一个和第二个单词对有最大的概率（P(y^<1>,y^<2>|x)）。按照条件概率的准则，这个可以表示成第一个单词的概率乘以第二个单词的概率，第二部分可以从紫色网络部分里得到（上图紫色所示），对于已经选择的in、jane、september这三个单词，你可以先保存P(y^<1>|x)这个概率值，然后再乘以第二个概率值就得到了第一个和第二个单词对的概率（P(y^<1>,y^<2>|x)）。



现在我们已经知道在第一个单词是in的情况下如何评估第二个单词的概率，当第一个单词是jane时，如上图第二行所示，同理，句子可能是"jane a"、"jane aaron"，...，"jane is"、"jane visits"等等。用这个新的网络部分，y^<1>连接jane（紫色曲线），得到给定输入x和第一个词是jane下，第二个单词的概率，同理，可以乘以P(y^<1>|x)得到P(y^<1>,y^<2>|x)。

如上图第三行所示，最后对于单词september也一样，从单词a到单词zulu，计算出相应的概率。总的来说，对于集束搜索的第二步，因为这里集束宽为3，词汇表里有10,000个单词，那么最终我们会有3*10,000=30,000个可能的结果，就是集束宽乘以词汇表大小，我们要做的就是评估这30,000个选择，选出概率大的前三个。假如这30,000个选择里最可能的是“in September”、“jane is”和“jane visits”（上图红色标记），集束搜索算法会保存这些结果，然后用于下一次集束搜索。



在我们进入集束搜索的第三步之前，注意一下集束宽B=3，每一步我们都复制3个，同样用这种网络来评估部分句子和最后的结果，由于集束宽等于3，我们有三个网络副本（上图橘色标记），每个网络的第一个单词不同，而这三个网络可以高效地评估第二个单词所有的30,000个选择。所以不需要初始化30,000个网络副本，只需要这3个网络的副本就可以快速评估softmax的输出，即y^<2>的10,000个结果。



（3）让我们快速解释一下集束搜索的下一步，给定输入x（法语句子），y^<1>和y^<2>的概率值和前面一样，现在我们考虑第三个单词是什么，可以是“in September a”，“in September aaron”，...，“in September zulu”。为了评估第三个单词可能的选择，我们用这个网络部分（上图第一行），第一单词是in，第二个单词是september，所以这个网络部分可以用来评估在给定输入的法语句子x和给定的英语输出的前两个单词“in September”情况下，第三个单词的概率。对于“jane is”和“jane visits”也一样，然后集束搜索还是会挑选出针对前三个词的三个最可能的选择，可能是“in september jane”、“Jane is visiting”或者“Jane visits Africa”（红色标记）。

（4）然后继续进行集束搜索的第四步，过程同上，最终这个过程的输出一次增加一个单词，集束搜索最终会找到“Jane visits africa in september”这个句子，终止在句尾符号（SEP），算法会发现这是最有可能输出的一个英语句子。注意如果集束宽等于1，意味着只考虑1种可能结果，这实际上就变成了贪婪搜索算法。如果同时考虑多个，可能的结果比如3个，10个或者其他的个数，集束搜索通常会找到比贪婪搜索更好的输出结果。

### 



```py
import torch
from transformers import BertTokenizer, BertForMaskedLM
import torch.nn.functional as F

# 初始化BertTokenizer和预训练BERT模型
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForMaskedLM.from_pretrained('bert-base-chinese')

# 加载之前保存的模型权重
model.load_state_dict(torch.load('Bret-Model-hard.pt'))
model.eval()

# 设置生成文本的起始文本
starting_text = "在方府的一处别院当中，空旷的校场上，"

# 将起始文本编码为模型输入
input_ids = tokenizer.encode(starting_text, return_tensors='pt')

# 设置beam搜索的宽度
beam_width = 5

# 使用beam搜索来生成续写文本
with torch.no_grad():
    for _ in range(50):  # 生成50个词
        outputs = model(input_ids)
        logits = outputs.logits[:, -1, :]  # 获取最后一个位置的logits
        next_word_probs = F.softmax(logits, dim=-1)  # 对logits进行softmax得到下一个词的概率
        next_word_probs, next_word_indices = next_word_probs.topk(beam_width, dim=1)  # 选择beam_width个候选词
        next_word_probs = next_word_probs.log()  # 取对数得到概率值
        input_ids = torch.cat([input_ids.repeat(beam_width, 1), next_word_indices], dim=1)  # 将新的候选词拼接到输入中并返回进行重新生成

# 将模型输出的token ids转换为文本并打印出来
generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
print(generated_text)

```



### 4.2使用多项式抽样进行文本生成（效果略好一点）

这个效果比起前面beam方法效果好了不少，至少没有单一重复字符与生僻字了

```py
import torch
from transformers import BertTokenizer, BertForMaskedLM

def generate_novel_continuation_with_sentence_repetition_penalty(model_path, seed_text, max_length=20, temperature=1.0, repetition_penalty=2.0):
    # 加载模型和tokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    model = BertForMaskedLM.from_pretrained("bert-base-chinese")
    #model.load_state_dict(torch.load(model_path))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    # 将seed_text编码为input_ids和attention_mask
    input_ids = tokenizer.encode(seed_text, add_special_tokens=True, return_tensors="pt", padding=True).to(device)
    attention_mask = torch.ones_like(input_ids).to(device)

    # 记录已生成的字符
    generated_tokens = set(input_ids.view(-1).tolist())

    with torch.no_grad():
        for _ in range(max_length):
            # 正向传播，获取模型输出
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits[:, -1, :] / temperature

            # 计算重复字符的惩罚
            for token_id in generated_tokens:
                logits[0, token_id] /= repetition_penalty

            # 通过logits计算下一个词的概率分布
            next_token_probs = torch.softmax(logits, dim=-1)

            # 对概率分布进行多项式抽样
            next_token_id = torch.multinomial(next_token_probs, num_samples=1)

            # 将新的词添加到输入中，用于继续生成
            input_ids = torch.cat([input_ids, next_token_id], dim=1)
            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1)

            # 更新已生成的字符集合
            generated_tokens.add(next_token_id.item())

    # 解码生成的文本
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)

    return generated_text

# 调用续写函数
#model_path = "Bret-Model-hard.pt"
#model_path = "Bret-Model-simple.pt"
#model_path = "ces-BretModelN-GRAM.pt"
#model_path ="BretModelN-GRAM.pt"
seed_text = "后来据说那个男人带她去吃消夜，连回家的出租车费都是自己付的。之后还常常会想他，希望能再见到他。"
generated_text = generate_novel_continuation_with_sentence_repetition_penalty(model_path, seed_text, temperature=0.7, repetition_penalty=3.0)
           #这里的温度是控制文本生成多样性的，这里的repetition_penalty是控制文本重复度生成的
print(generated_text)

```



## 5反思与新的模型

在多次长时间的训练模型与使用中，我意识到bert-base-Chinese作为bert预模型，尝试将其直接训练并应用于文本生成任务(Natural Language Generation)的结果是不甚理想的。究其原因，是在于预训练阶段和下游任务阶段的差异。他的侧重点是在encoder上，所以他的上下文预测字任务完成的十分良好，但是句子生成任务就不甚理想



我在网上冲浪的时候找到了一篇论文Leveraging Pre-trained Checkpoints for Sequence Generation Tasks

[2020.tacl-1.18.pdf (aclanthology.org)](https://aclanthology.org/2020.tacl-1.18.pdf)，他研究了把BERT、RoBERTa、GPT2三个模型混合起来去做生成，用BERT初始化encoder和decoder的方法，然后用GPT2进行中间预测。

- **GPT**是一种Auto-Regressive(自回归)的语言模型。它可以看作是Transformer model的Decoder部分，它的优化目标就是标准的语言模型目标：序列中所有token的联合概率。GPT采用的是自然序列中的从左到右（或者从右到左）的因式分解。
- **RoBERTa**（A Robustly Optimized BERT Pretraining Approach）是一种基于**BERT**（Bidirectional Encoder Representations from Transformers）的改进型语言表示模型，由Facebook AI研究团队提出。RoBERTa 旨在优化 BERT 的训练方式，从而获得更加鲁棒和高效的预训练模型。
- **BERT**是一种Auto-Encoding(自编码)的语言模型。它可以看作是Transformer model的Encoder部分，在输入端随机使用一种特殊的[MASK]token来替换序列中的token，这也可以看作是一种noise，所以BERT也叫Masked Language Model。

那么我可不可以参考这一做法，由BERT初始化encoder和decoder，然后借由GPT模型进行理解并生成文本呢？

于是我写了以下代码：

### 5.1bert-GPT复合模型建立与使用



```py
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer,  BertForMaskedLM, AutoTokenizer,AutoModelWithLMHead
import pickle


# 定义 FictionDataset 类（同上n-gram方法）
class FictionDataset(Dataset):
    def __init__(self, txt_file, max_length=64, n_gram=8):
        self.data = []
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.max_length = max_length
        self.n_gram = n_gram

        with open(txt_file, "r", encoding="utf-8") as f:
            text = f.read().replace("\n", "")
            start_idx = 0
            while start_idx < len(text):
                chunk = text[start_idx: start_idx + self.max_length]
                self.data.append(chunk)
                start_idx += self.max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]

        samples = []
        for i in range(len(text) - self.n_gram):
            sample_text = text[i:i + self.n_gram]
            encoded = self.tokenizer.encode_plus(sample_text, max_length=self.max_length, padding='max_length',
                                                 truncation=True, return_tensors='pt')
            input_ids = encoded['input_ids'].squeeze()
            attention_mask = encoded['attention_mask'].squeeze()
            token_type_ids = encoded['token_type_ids'].squeeze()

            samples.append({
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'token_type_ids': token_type_ids
            })

        return samples


dataset_save_path = "Ces-Dataset.pkl"

txt_file_path = "综合.txt"
dataset = FictionDataset(txt_file=txt_file_path, max_length=128)

with open(dataset_save_path, 'wb') as f:
    pickle.dump(dataset, f)



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#加载复合模型
gpt2_tokenizer = AutoTokenizer.from_pretrained('uer/gpt2-chinese-cluecorpussmall')
gpt2_model = AutoModelWithLMHead.from_pretrained('uer/gpt2-chinese-cluecorpussmall')
gpt2_model.to(device)


bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
bert_model = BertForMaskedLM.from_pretrained('bert-base-chinese')
bert_model.to(device)



batch_size = 8
learning_rate = 2e-5
epochs = 4

optimizer = torch.optim.Adam(list(bert_model.parameters()) + list(gpt2_model.parameters()), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

with open(dataset_save_path, 'rb') as f:
    dataset = pickle.load(f)

dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 联合训练
for epoch in range(epochs):
    bert_model.train()
    gpt2_model.train()
    total_loss = 0

    for batch_idx, batch in enumerate(dataloader):
        batch_samples = batch  # 每个批次都是一个样本（字典）列表

        for sample_idx, sample in enumerate(batch_samples):
            input_ids = sample['input_ids'].to(device)
            attention_mask = sample['attention_mask'].to(device)

            # BERT部分
            bert_labels = input_ids.clone()
            bert_labels[bert_labels == bert_tokenizer.pad_token_id] = -100  # 忽略[PAD]的标记

            optimizer.zero_grad()
            bert_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=bert_labels)
            bert_loss = bert_outputs.loss
            bert_loss.backward()

            # GPT-2部分
            gpt2_input_ids = input_ids[:, :-1].to(device)  # 将最后一个标记移除
            gpt2_labels = input_ids[:, 1:].contiguous().to(device)  # 偏移输入变为标签

            gpt2_outputs = gpt2_model(input_ids=gpt2_input_ids, labels=gpt2_labels)
            gpt2_loss = gpt2_outputs.loss
            gpt2_loss.backward()

            optimizer.step()

            total_loss += (bert_loss.item() + gpt2_loss.item())  #计算复合损失
            if (sample_idx + 1) % 50==1:
                print(f"Batch [{batch_idx + 1}/{len(dataloader)}], Sample [{sample_idx + 1}/{len(batch_samples)}], Batch Loss: {bert_loss.item() + gpt2_loss.item():.4f}")


    print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}")

torch.save({
    'bert_model_state_dict': bert_model.state_dict(),
    'gpt2_model_state_dict': gpt2_model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, 'bert-gpt.pt')


```







### 5.2bert-GPT复合模型调用与输出

这里原有的直接输出函数生成效果并不是很好，我换了一个自定义的函数generate_text

这里的输出并不固定，可能需要几次实验才能达到结果。不过效果不错

```py
checkpoint = torch.load('bert-gpt.pt')
bert_model.load_state_dict(checkpoint['bert_model_state_dict'])
gpt2_model.load_state_dict(checkpoint['gpt2_model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

bert_model.eval()
gpt2_model.eval()
text_prompt = "贝冰榆仰头轻笑了一声，嘲讽的看着面前的中年女人，她觉得这个女人脑袋构造完全不能用正常人来判断"
max_length = 110
temperature = 0.7

#直接的输出函数（使用bert做encode与decode）
input_ids = bert_tokenizer.encode(text_prompt, add_special_tokens=True, return_tensors="pt").to(device)
output = gpt2_model.generate(input_ids, max_length=max_length, num_return_sequences=1)

generated_text = bert_tokenizer.decode(output[0], skip_special_tokens=True)
print("Generated Text:", generated_text)

#自定义的输出函数
def generate_text(model, tokenizer, input_ids, max_length, temperature, no_repeat_ngram_size):
    generated_ids = input_ids.clone()#克隆一下生成的文本编码序列
    for _ in range(max_length):
        logits = model(input_ids=generated_ids).logits[:, -1, :] / temperature# 使用给定的预训练语言模型 model 预测下一个 token 的 logits。generated_ids 是当前已生成的 token 序列，model 使用这些 token 进行预测。通过除以 temperature 来调整生成的随机性。
        next_token_id = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)
		#通过对 logits 应用 softmax 函数并使用多项式分布采样，从概率分布中随机选择一个 token 作为下一个 token。
        # 检查no_repeat_ngram的约束
        if no_repeat_ngram_size > 0 and len(generated_ids[0]) >= no_repeat_ngram_size:
            for _ in range(no_repeat_ngram_size):#进入一个循环，检查生成的 token 是否符合 no_repeat_ngram_size 约束。
                if torch.equal(generated_ids[0, -no_repeat_ngram_size:], next_token_id):
                    next_token_id = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)#检查是否出现连续的token
                    break

        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)#将刚刚生成的 next_token_id 连接到已生成的 token 序列中，更新 generated_ids

    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return generated_text

generated_text1 = generate_text(gpt2_model, bert_tokenizer, input_ids, max_length, temperature=0.7, no_repeat_ngram_size=1)
print("Generated Text2:", generated_text1)

print("111")

```

以下附上一下与直接生成文本的对比：

![测试1](C:\Users\Lenovo\Desktop\测试1.png)

## 番外

不久前听大佬说Bert2Bert也可以用，在这里附上我简单写的代码（需要微调一下数据集）：

### 什么是bert2bert

`bert2bert` 不是一个特定的模型，而是一种通用的框架，用于构建基于 BERT（Bidirectional Encoder Representations from Transformers）的 Seq2Seq（序列到序列）模型。在 Seq2Seq 模型中，输入是一个序列，而输出也是一个序列，这种模型广泛应用于机器翻译、摘要生成、对话生成等任务。

`bert2bert` 是在 BERT 的基础上构建的 Seq2Seq 模型。具体来说，它使用一个 BERT 编码器（Encoder）来处理输入序列，然后使用一个 BERT 解码器（Decoder）来生成输出序列。这样的架构允许模型在输入和输出之间建立语义关联，从而实现各种文本生成任务。

在 `bert2bert` 中，编码器和解码器可以共享同样的 BERT 参数，也可以使用不同的 BERT 模型。例如，你可以使用一个中文 BERT 作为编码器，然后使用一个英文 BERT 作为解码器，从而实现中英文翻译任务。或者你可以使用相同的 BERT 模型作为编码器和解码器，实现文本摘要生成任务。

### bert2bert模型

```py
import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer, EncoderDecoderModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, default_data_collator
import numpy as np
import torch.nn as nn

# 构建 FictionDataset 类（根据您的数据集）
class FictionDataset(Dataset):
    def __init__(self, txt_file, max_length=128):
        self.data = []  # 存储文本数据的列表
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.max_length = max_length

        # 从TXT文件中读取数据(并尽可能填充长度至128)
        with open(txt_file, "r", encoding="utf-8") as f:
            text = f.read().replace("\n", "")  # 移除换行符
            start_idx = 0
            while start_idx < len(text):
                chunk = text[start_idx : start_idx + self.max_length]
                self.data.append(chunk)
                start_idx += self.max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]

        # 使用BertTokenizer对文本进行分词，并添加特殊标记[CLS]和[SEP]
        # truncation=True 表示截断文本以适配指定的max_length
        # padding='max_length' 表示对文本进行填充以适配指定的max_length
        # return_tensors='pt' 表示返回PyTorch张量
        encoded = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length',truncation=True, return_tensors='pt')

        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        token_type_ids = encoded['token_type_ids'].squeeze()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
            'labels': input_ids,  # 注意这里将输入的input_ids作为目标序列
            'label_ids': input_ids  # 添加 label_ids，表示要预测的下一个 token
        }


class NextTokenLoss(nn.Module):
    def __init__(self):
        super(NextTokenLoss, self).__init__()

    def forward(self, logits, labels):
        # 根据 labels 的形状获取要预测的下一个 token 位置
        next_token_positions = (labels != -100).nonzero(as_tuple=False)

        # 根据预测的下一个 token 位置，提取相应位置的 logits
        batch_indices = next_token_positions[:, 0]
        token_indices = next_token_positions[:, 1]
        predicted_logits = logits[batch_indices, token_indices]

        # 使用 nn.CrossEntropyLoss 计算损失
        loss_fn = nn.CrossEntropyLoss()
        loss = loss_fn(predicted_logits, labels.view(-1))  # 将 labels 调整为一维向量

        return loss


class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    def compute_loss(self, model, inputs):
        labels = inputs["labels"]
        outputs = model(**inputs)  						#将label转化为输入
        logits = outputs.logits

        loss_fn = NextTokenLoss() 						 # 自定义的损失函数
        loss = loss_fn(logits, labels)
        return loss

# 加载 FictionDataset
dataset = FictionDataset(txt_file="综合.txt", max_length=128)


# 初始化 BertTokenizer 和 EncoderDecoderModel
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-chinese", "bert-base-chinese")

# 设置decoder_start_token_id和pad_token_id属性
bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id
bert2bert.config.pad_token_id = tokenizer.pad_token_id


# 将模型放置在CPU上
device = torch.device("cpu")
bert2bert.to(device)

# 加载训练好的权重
model_checkpoint = "bert2bert-model.bin"
bert2bert.load_state_dict(torch.load(model_checkpoint, map_location=device))
bert2bert.eval()

''''''

# 训练参数和配置
training_args = Seq2SeqTrainingArguments(
    output_dir="./output",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="./logs",
    fp16=True  # 启用半精度训练
)

# 构建 Trainer 对象并进行训练
trainer = CustomSeq2SeqTrainer(
    model=bert2bert,
    args=training_args,
    data_collator=default_data_collator,  # 默认的数据整合器用于处理Seq2Seq数据
    train_dataset=dataset,
    compute_metrics=None  # 禁用默认的评估指标计算
)
trainer.train()

save_path = "bert2bert-model2.bin"
torch.save(bert2bert.state_dict(), save_path)
print("Model weights saved to:", save_path)


print('666')

#使用束搜索生成
def generate_text_with_beam_search(prompt,bad_words, max_length=60, temperature=0.7, top_k=50, top_p=0.95, beam_size=7):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    input_ids = input_ids.to(device)

    output = bert2bert.generate(
        input_ids=input_ids,
        max_length=max_length,
        num_return_sequences=beam_size,  # 使用Beam Search
        no_repeat_ngram_size=3,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        num_beams=beam_size,  # 设置Beam Size
        bad_words_ids = bad_words  # 添加禁止词列表，免得生成一些奇怪的文字
    )

    generated_texts = []

    for seq in output:
        generated_text = tokenizer.decode(seq, skip_special_tokens=True)
        generated_texts.append(generated_text)

    return generated_texts

# 生成文本示例
text = "单手持棒抵住进攻，一棒砸过去，另一只手则蓄积法力，随时准备偷袭；六耳猕猴见状，有样学样，也"

prompt_token_ids = tokenizer.encode(text, add_special_tokens=False)
bad_words = [prompt_token_ids]
badWords = tokenizer.decode(prompt_token_ids, skip_special_tokens=True)
generated_texts = generate_text_with_beam_search(text,bad_words=bad_words)



for i, generated_text in enumerate(generated_texts):
    print(f"Generated Text {i+1}:", text + generated_text)


```

### 改进（还没练）

前面实际文本生成过于单一，且句子只能在有限的范围内进行预测并且大多参考原文，考虑可能是bert以上下文为基础预测所导致的。以下是对序列只考虑上文对下一个字预测的一些修改代码：

首先是数据集部分：

```py
    def __getitem__(self, idx):
        text = self.data[idx]

        encoded = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length',
                                             truncation=True, return_tensors='pt')

        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        token_type_ids = encoded['token_type_ids'].squeeze()

        # 根据输入文本，提取倒数第8个 token 的位置作为标签位置
        last_token_position = len(text) - 8

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'token_type_ids': token_type_ids,
            'labels': input_ids[last_token_position],  # 将倒数第8个 token 作为标签
        }
```

然后是自建loss函数与自建的train函数：

```py
class NextTokenLoss(nn.Module):
    def __init__(self):
        super(NextTokenLoss, self).__init__()

    def forward(self, logits, labels):
        # 根据 labels 的形状获取要预测的下一个 token 位置
        next_token_positions = (labels != -100).nonzero(as_tuple=False)

        # 提取每个序列中的最后一个 token 的预测 logits
        predicted_logits = logits[:, -8, :]

        # 使用 nn.CrossEntropyLoss 计算损失
        loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
        loss = loss_fn(predicted_logits.view(-1, predicted_logits.shape[-1]), labels.view(-1))

        return loss

# 构建 Trainer 类
class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    def compute_loss(self, model, inputs):
        labels = inputs["labels"]
        attention_mask = inputs["attention_mask"]

        ##print(inputs["input_ids"])
        #print(inputs["attention_mask"])

        # 根据上下文生成decoder_input_ids（删除倒数第8个token，因为它是标签）
        decoder_input_ids = inputs["input_ids"][:, :-8]

        # 对decoder_input_ids进行移位以匹配标签的形状
        shifted_decoder_input_ids = decoder_input_ids.clone()
        shifted_decoder_input_ids[:, 1:] = decoder_input_ids[:, :-1]

        outputs = model(input_ids=inputs["input_ids"], attention_mask=attention_mask,
                        decoder_input_ids=shifted_decoder_input_ids)
        logits = outputs.logits

        loss_fn = NextTokenLoss()
        loss = loss_fn(logits, labels)
        return loss
```

这样训练完之后可能效果会更好一些

